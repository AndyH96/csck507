{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f69ce32",
   "metadata": {},
   "source": [
    "# CSCK507 Mid-Module Assignment\n",
    "### Toxic comment classification challenge\n",
    "\n",
    "## Table of Contents\n",
    "[Section 1. Introduction](#introduction)\n",
    "- [Import Dependencies](#import-dependencies)\n",
    "\n",
    "[Section 2. Data Exploration & Analysis](#data-exploration-&-analysis)\n",
    "  - [2.1 Data Preprocessing](#data-preprocessing)\n",
    "  - [2.2 Data Imbalance](#data-imbalance)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "363c45f8",
   "metadata": {},
   "source": [
    "## 1. Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9da7374",
   "metadata": {},
   "source": [
    "### Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d3aae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# For Data Preprocessing\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re  \n",
    "\n",
    "# For Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "\n",
    "#For Feature Extraction  \n",
    "import spacy\n",
    "import string \n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer   \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdd219e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into pandas DataFrame with relative path\n",
    "\n",
    "df = pd.read_csv('./train.csv')\n",
    "df_test = pd.read_csv('./test_labels.csv')\n",
    "df_testcomments = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5510ef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spacy.prefer_gpu()\n",
    "    spacy.load('en_core_web_sm')\n",
    "except LookupError:\n",
    "    print('Run: python -m spacy download en_core_web_sm')\n",
    "\n",
    "try:\n",
    "    nltk_stop = stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c555deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise SpaCy Model \n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d042ae66",
   "metadata": {},
   "source": [
    "## 2. Data Exploration & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1957dae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic\n",
      "0    144277\n",
      "1     15294\n",
      "Name: count, dtype: int64\n",
      "\n",
      "severe_toxic\n",
      "0    157976\n",
      "1      1595\n",
      "Name: count, dtype: int64\n",
      "\n",
      "obscene\n",
      "0    151122\n",
      "1      8449\n",
      "Name: count, dtype: int64\n",
      "\n",
      "threat\n",
      "0    159093\n",
      "1       478\n",
      "Name: count, dtype: int64\n",
      "\n",
      "insult\n",
      "0    151694\n",
      "1      7877\n",
      "Name: count, dtype: int64\n",
      "\n",
      "identity_hate\n",
      "0    158166\n",
      "1      1405\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "for column in class_columns:\n",
    "    class_counts = df[column].value_counts()\n",
    "    print(class_counts)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3e85c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   id             159571 non-null  object\n",
      " 1   comment_text   159571 non-null  object\n",
      " 2   toxic          159571 non-null  int64 \n",
      " 3   severe_toxic   159571 non-null  int64 \n",
      " 4   obscene        159571 non-null  int64 \n",
      " 5   threat         159571 non-null  int64 \n",
      " 6   insult         159571 non-null  int64 \n",
      " 7   identity_hate  159571 non-null  int64 \n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 9.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53bf8b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing -1: (153164, 7)\n",
      "After removing -1: (63978, 7)\n"
     ]
    }
   ],
   "source": [
    "# obtain class labels of the dataset\n",
    "class_labels = list(df.columns[2:])\n",
    "class_labels\n",
    "\n",
    "# remove rows with -1 from df_test as it is not used for scoring\n",
    "print(f'Before removing -1: {df_test.shape}')\n",
    "for class_label in class_labels:\n",
    "    df_test = df_test[df_test[class_label] != -1]\n",
    "print(f'After removing -1: {df_test.shape}')\n",
    "\n",
    "# left join 'df_test' and 'df_testcomments' on 'id' column\n",
    "df_test = pd.merge(df_test, df_testcomments, on='id', how='left')\n",
    "\n",
    "# rearraange columns to be the same as df\n",
    "df_test = df_test[['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7feae058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   id             159571 non-null  object\n",
      " 1   comment_text   159571 non-null  object\n",
      " 2   toxic          159571 non-null  int64 \n",
      " 3   severe_toxic   159571 non-null  int64 \n",
      " 4   obscene        159571 non-null  int64 \n",
      " 5   threat         159571 non-null  int64 \n",
      " 6   insult         159571 non-null  int64 \n",
      " 7   identity_hate  159571 non-null  int64 \n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 9.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f2fd03",
   "metadata": {},
   "source": [
    "### 2.1 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b50e5572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess a text string.\n",
    "\n",
    "    Operations performed:\n",
    "    - Replace special characters, URLs, and numbers with spaces.\n",
    "    - Remove extra spaces and replace \"\\n\" with a space.\n",
    "    - Remove Non-English characters.\n",
    "    - Remove start and end white spaces.\n",
    "    - Remove single characters.\n",
    "    - Remove punctuations.\n",
    "    - Convert the text to lowercase.\n",
    "    - Remove common stopwords.\n",
    "\n",
    "    :param text: Input text (string).\n",
    "    :return: Cleaned text (string).\n",
    "\n",
    "    Example:\n",
    "    >>> input_text = \"An example text with special characters: $100 and URLs like https://example.com.\"\n",
    "    >>> preprocess_text(input_text)\n",
    "    'example text special characters URLs like'\n",
    "    \"\"\"\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    # Remove extra spaces and replace \"\\n\" with a space\n",
    "    text = re.sub(\"\\s\\s+\", \" \", text).replace(\"\\n\", \" \")\n",
    "    # Remove Non-English characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', \"\", text)\n",
    "    # Remove start and end white spaces\n",
    "    text = text.strip()\n",
    "    # Remove single characters\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", text)\n",
    "    # Remove punctuations\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]+\", \" \", text)\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Stopword Removal\n",
    "    text = ' '.join([word for word in text.split() if word not in nltk_stop])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e90b740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(documents):\n",
    "    \"\"\"\n",
    "    Tokenize a list of documents and perform the following:\n",
    "    1. Break text into individual words or subword tokens.\n",
    "    2. Reduce words to their base or root form using lemmatization.\n",
    "    3. Remove stop words and non-alphabetic characters.\n",
    "\n",
    "    Using spaCy's nlp.pipe to batch process texts and yield Doc objects.\n",
    "\n",
    "    :param documents: List of strings representing documents.\n",
    "    :return: List of lists of strings, where each list corresponds to the lemmatized tokens of a document.\n",
    "\n",
    "    Example:\n",
    "    >>> input_documents = [\"Tokenize this document.\", \"And tokenize another one.\"]\n",
    "    >>> tokenize_text(input_documents)\n",
    "    [['tokenize', 'document'], ['tokenize']]\n",
    "    \"\"\"\n",
    "    lemma_list = []\n",
    "    # Disable \"ner\" and \"parser\" components for faster processing\n",
    "    for doc in nlp.pipe(documents, disable=[\"ner\", \"parser\"], batch_size=1000):\n",
    "        # Generate lemmatized tokens\n",
    "        lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "        # Remove stop words and non-alphabetic characters\n",
    "        lemmatized_tokens = [token for token in lemmatized_tokens\n",
    "                             if token not in nlp.Defaults.stop_words\n",
    "                             and token.isalpha()]\n",
    "        lemma_list.append(lemmatized_tokens)\n",
    "\n",
    "    return lemma_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e1c7971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>d aww he matches this background colour i m se...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>hey man i m really not trying to edit war it s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>more can t make any real suggestions on impro...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00025465d4725e87</td>\n",
       "      <td>congratulations from me as well use the tools...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>cocksucker before you piss around on my work</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00031b1e95af7921</td>\n",
       "      <td>your vandalism to the matt shirvington article...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00037261f536c51d</td>\n",
       "      <td>sorry if the word nonsense was offensive to yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00040093b2687caa</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  explanation why the edits made under my userna...      0   \n",
       "1  000103f0d9cfb60f  d aww he matches this background colour i m se...      0   \n",
       "2  000113f07ec002fd  hey man i m really not trying to edit war it s...      0   \n",
       "3  0001b41b1c6bb37e   more can t make any real suggestions on impro...      0   \n",
       "4  0001d958c54c6e35  you sir are my hero any chance you remember wh...      0   \n",
       "5  00025465d4725e87   congratulations from me as well use the tools...      0   \n",
       "6  0002bcb3da6cb337       cocksucker before you piss around on my work      1   \n",
       "7  00031b1e95af7921  your vandalism to the matt shirvington article...      0   \n",
       "8  00037261f536c51d  sorry if the word nonsense was offensive to yo...      0   \n",
       "9  00040093b2687caa  alignment on this subject and which are contra...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  \n",
       "5             0        0       0       0              0  \n",
       "6             1        1       0       1              0  \n",
       "7             0        0       0       0              0  \n",
       "8             0        0       0       0              0  \n",
       "9             0        0       0       0              0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply preprocessing to train data\n",
    "df['comment_text'] = df['comment_text'].apply(preprocess_text)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2645ebab",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Create a TF-IDF (similar to bag of words but it provides importance based on word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
