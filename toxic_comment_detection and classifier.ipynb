{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f69ce32",
   "metadata": {},
   "source": [
    "# CSCK507 Mid Module - Toxic Comment Classification Challenge\n",
    "\n",
    "## Table of Contents\n",
    "[Section 1. Introduction](#introduction)\n",
    "- [Import Dependencies](#import-dependencies)\n",
    "- [Initialise SpaCy Model ](#import-dependencies)\n",
    "\n",
    "[Section 2. Data Exploration & Analysis](#data-exploration-&-analysis)\n",
    "  - [Dataset Alignment](#data-preprocessing)\n",
    "  - [Data Preprocessing](#data-preprocessing)\n",
    "  - [Tokenisation & Lemmatisation](#data-preprocessing)\n",
    "  \n",
    "    [Section 2.1 Requirements](#data-exploration-&-analysis)\n",
    "  - [Number of Sentences & Tokens Per Class](#data-preprocessing)\n",
    "  - [Data Imbalance](#data-preprocessing)\n",
    "  - [Understanding the Most Common Words](#data-preprocessing)\n",
    "\n",
    "  \n",
    "  - [Data Imbalance](#data-imbalance)\n",
    "  \n",
    "[Section 3. Feature Extraction](#data-exploration-&-analysis)\n",
    "\n",
    "[Section 4. Machine Learning Models](#data-exploration-&-analysis)\n",
    "\n",
    "[Section 5. Model Evaluation](#data-exploration-&-analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a35d39a",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Introduction\n",
    "\n",
    "Originating in 2018, this challenge revolves around classifying different levels of toxicity in online comments. The dataset from the inaugural competition is utilized to analyze and evaluate the performance of various machine learning algorithms in categorizing six types of toxicity. The primary goal is not only to find an optimal solution but to understand the process of evaluating machine learning algorithms' performance in a classification task. This individual assessment involves data analysis, algorithm selection, and the exploration of feature extraction methods to uncover insights into the nuances of toxic comment classification.\n",
    "\n",
    "The Toxic Comment Classification Challenge and dataset can be obtained from Kaggle, here: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f040f411",
   "metadata": {},
   "source": [
    "### Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "4d3aae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import spacy\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# For Data Preprocessing\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re  \n",
    "\n",
    "# For Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#For Feature Extraction  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string \n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ab349f",
   "metadata": {},
   "source": [
    "### Loading Kaggle dataset into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "cdd219e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./train.csv')\n",
    "df_test_labels = pd.read_csv('./test_labels.csv')\n",
    "df_test_comment = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a5a595",
   "metadata": {},
   "source": [
    "### Initialise SpaCy Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "5510ef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spacy.prefer_gpu()\n",
    "    spacy.load('en_core_web_sm')\n",
    "except LookupError:\n",
    "    print('Run: python -m spacy download en_core_web_sm')\n",
    "\n",
    "try:\n",
    "    nltk_stop = stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "9716bf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d038ddd4",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Exploration & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "219d9303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "e3e85c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   id             159571 non-null  object\n",
      " 1   comment_text   159571 non-null  object\n",
      " 2   toxic          159571 non-null  int64 \n",
      " 3   severe_toxic   159571 non-null  int64 \n",
      " 4   obscene        159571 non-null  int64 \n",
      " 5   threat         159571 non-null  int64 \n",
      " 6   insult         159571 non-null  int64 \n",
      " 7   identity_hate  159571 non-null  int64 \n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 9.7+ MB\n",
      "The table dimensions are: (159571, 8)\n"
     ]
    }
   ],
   "source": [
    "df.info()\n",
    "print(\"The table dimensions are:\",df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54b6403",
   "metadata": {},
   "source": [
    "### Aligning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "53bf8b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_test before removing -1: (153164, 8)\n",
      "df_test after removing -1: (63978, 7)\n"
     ]
    }
   ],
   "source": [
    "# Obtain class labels of the dataset\n",
    "class_labels = list(df.columns[2:])\n",
    "class_labels\n",
    "\n",
    "# Remove rows with -1 from df_test as they are not used for scoring\n",
    "print(f'df_test before removing -1: {df_test.shape}')\n",
    "for class_label in class_labels:\n",
    "    df_test = df_test_labels[df_test_labels[class_label] != -1]\n",
    "print(f'df_test after removing -1: {df_test.shape}')\n",
    "\n",
    "# Left join 'df_test' and 'df_test_comment' on the 'id' column\n",
    "df_test = pd.merge(df_test_labels, df_test_comment, on='id', how='left')\n",
    "\n",
    "# Rearrange columns to match the structure of 'df'\n",
    "df_test = df_test[['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d2825a",
   "metadata": {},
   "source": [
    "### 2.1 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "b50e5572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess a text string.\n",
    "\n",
    "    Operations performed:\n",
    "    - Replace special characters, URLs, and numbers with spaces.\n",
    "    - Remove extra spaces and replace \"\\n\" with a space.\n",
    "    - Remove Non-English characters.\n",
    "    - Remove start and end white spaces.\n",
    "    - Remove single characters.\n",
    "    - Remove punctuations.\n",
    "    - Convert the text to lowercase.\n",
    "    - Remove common stopwords.\n",
    "\n",
    "    :param text: Input text (string).\n",
    "    :return: Cleaned text (string).\n",
    "\n",
    "    Example:\n",
    "    >>> input_text = \"An example text with special characters: $100 and URLs like https://example.com.\"\n",
    "    >>> preprocess_text(input_text)\n",
    "    'example text special characters URLs like'\n",
    "    \"\"\"\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    # Remove extra spaces and replace \"\\n\" with a space\n",
    "    text = re.sub(\"\\s\\s+\", \" \", text).replace(\"\\n\", \" \")\n",
    "    # Remove Non-English characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', \"\", text)\n",
    "    # Remove start and end white spaces\n",
    "    text = text.strip()\n",
    "    # Remove single characters\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", text)\n",
    "    # Remove punctuations\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]+\", \" \", text)\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Stopword Removal\n",
    "    text = ' '.join([word for word in text.split() if word not in nltk_stop])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd1f0df",
   "metadata": {},
   "source": [
    "This part consists of general cleaning\n",
    "regex for extra characters, stop words, lowercasing, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d0c244",
   "metadata": {},
   "source": [
    "### Tokenisation and Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "e9bb6e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(documents):\n",
    "    \"\"\"\n",
    "    Tokenize a list of documents and perform the following:\n",
    "    1. Break text into individual words or subword tokens.\n",
    "    2. Reduce words to their base or root form using lemmatization.\n",
    "    3. Remove stop words and non-alphabetic characters.\n",
    "\n",
    "    Utilises spaCy's nlp.pipe for efficient batch processing.\n",
    "\n",
    "    :param documents: List of strings representing documents.\n",
    "    :return: List of lists of strings, where each list corresponds to the lemmatized tokens of a document.\n",
    "    \"\"\"\n",
    "    lemmatized_tokens_list = []\n",
    "    \n",
    "    # Process documents using spaCy's nlp.pipe with \"ner\" and \"parser\" components disabled\n",
    "    for doc in nlp.pipe(documents, disable=[\"ner\", \"parser\"], batch_size=5000):\n",
    "        # Generate lemmatised tokens, remove stop words, and non-alphabetic characters\n",
    "        lemmatized_tokens = [token.lemma_ for token in doc if token.is_alpha and token.lemma_ not in nlp.Defaults.stop_words]\n",
    "        lemmatized_tokens_list.append(lemmatized_tokens)\n",
    "\n",
    "    return lemmatized_tokens_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4c76cc",
   "metadata": {},
   "source": [
    "In many tokenization tasks, especially when you're primarily interested in lemmatization and removing stop words, you may not need the additional information provided by the \"ner\" and \"parser\" components.\n",
    "\n",
    "Disabling the \"ner\" and \"parser\" components during the processing of documents with nlp.pipe will reduce computational laod and can significantly improve efficiency and speed, especially when dealing with a large amount of text data.\n",
    "\n",
    "It's a trade-off between computational resources and the specific linguistic information your task requires. If named entities and syntactic parsing are not critical for your task, disabling these components is a pragmatic approach to enhance processing speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "4e1c7971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>aww matches background colour seemingly stuck ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>make real suggestions improvement wondered sec...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00025465d4725e87</td>\n",
       "      <td>congratulations well use tools well talk</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>cocksucker piss around work</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00031b1e95af7921</td>\n",
       "      <td>vandalism matt shirvington article reverted pl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00037261f536c51d</td>\n",
       "      <td>sorry word nonsense offensive anyway intending...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00040093b2687caa</td>\n",
       "      <td>alignment subject contrary dulithgow</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  explanation edits made username hardcore metal...      0   \n",
       "1  000103f0d9cfb60f  aww matches background colour seemingly stuck ...      0   \n",
       "2  000113f07ec002fd  hey man really trying edit war guy constantly ...      0   \n",
       "3  0001b41b1c6bb37e  make real suggestions improvement wondered sec...      0   \n",
       "4  0001d958c54c6e35                      sir hero chance remember page      0   \n",
       "5  00025465d4725e87           congratulations well use tools well talk      0   \n",
       "6  0002bcb3da6cb337                        cocksucker piss around work      1   \n",
       "7  00031b1e95af7921  vandalism matt shirvington article reverted pl...      0   \n",
       "8  00037261f536c51d  sorry word nonsense offensive anyway intending...      0   \n",
       "9  00040093b2687caa               alignment subject contrary dulithgow      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  \n",
       "5             0        0       0       0              0  \n",
       "6             1        1       0       1              0  \n",
       "7             0        0       0       0              0  \n",
       "8             0        0       0       0              0  \n",
       "9             0        0       0       0              0  "
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply preprocessing to train data\n",
    "df['comment_text'] = df['comment_text'].apply(preprocess_text)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "5deb9a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>yo bitch ja rule succesful ever whats hating s...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>rfc title fine imo</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>sources zawe ashton lapland</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>look back source information updated correct f...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>anonymously edit articles</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0001ea8717f6de06</td>\n",
       "      <td>thank understanding think highly would revert ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00024115d4cbde0f</td>\n",
       "      <td>please add nonsense wikipedia edits considered...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000247e83dcc1211</td>\n",
       "      <td>dear god site horrible</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00025358d4737918</td>\n",
       "      <td>fool believe numbers correct number lies 10 00...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00026d1092fe71cc</td>\n",
       "      <td>double redirects fixing double redirects blank...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  00001cee341fdb12  yo bitch ja rule succesful ever whats hating s...     -1   \n",
       "1  0000247867823ef7                                 rfc title fine imo     -1   \n",
       "2  00013b17ad220c46                        sources zawe ashton lapland     -1   \n",
       "3  00017563c3f7919a  look back source information updated correct f...     -1   \n",
       "4  00017695ad8997eb                          anonymously edit articles     -1   \n",
       "5  0001ea8717f6de06  thank understanding think highly would revert ...      0   \n",
       "6  00024115d4cbde0f  please add nonsense wikipedia edits considered...     -1   \n",
       "7  000247e83dcc1211                             dear god site horrible      0   \n",
       "8  00025358d4737918  fool believe numbers correct number lies 10 00...     -1   \n",
       "9  00026d1092fe71cc  double redirects fixing double redirects blank...     -1   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0            -1       -1      -1      -1             -1  \n",
       "1            -1       -1      -1      -1             -1  \n",
       "2            -1       -1      -1      -1             -1  \n",
       "3            -1       -1      -1      -1             -1  \n",
       "4            -1       -1      -1      -1             -1  \n",
       "5             0        0       0       0              0  \n",
       "6            -1       -1      -1      -1             -1  \n",
       "7             0        0       0       0              0  \n",
       "8            -1       -1      -1      -1             -1  \n",
       "9            -1       -1      -1      -1             -1  "
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply preprocessing to new test data\n",
    "df_test['comment_text'] = df_test['comment_text'].apply(preprocess_text)\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b95c8d",
   "metadata": {},
   "source": [
    "### Count number of sentences and tokens per class using SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "44a87d4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[227], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Iterate through comments in the current class\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m comment \u001b[38;5;129;01min\u001b[39;00m class_comments:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Process the comment with spaCy\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     doc \u001b[38;5;241m=\u001b[39m nlp(comment)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Count sentences and tokens\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     total_sentences \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(doc\u001b[38;5;241m.\u001b[39msents))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/spacy/language.py:1011\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1011\u001b[0m     doc \u001b[38;5;241m=\u001b[39m proc(doc, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcomponent_cfg\u001b[38;5;241m.\u001b[39mget(name, {}))  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/spacy/pipeline/trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/spacy/pipeline/transition_parser.pyx:253\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.predict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/spacy/pipeline/transition_parser.pyx:274\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.greedy_parse\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/thinc/model.py:315\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[1;32m    312\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/spacy/ml/tb_framework.py:33\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(model, X, is_train):\n\u001b[0;32m---> 33\u001b[0m     step_model \u001b[38;5;241m=\u001b[39m ParserStepModel(\n\u001b[1;32m     34\u001b[0m         X,\n\u001b[1;32m     35\u001b[0m         model\u001b[38;5;241m.\u001b[39mlayers,\n\u001b[1;32m     36\u001b[0m         unseen_classes\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munseen_classes\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     37\u001b[0m         train\u001b[38;5;241m=\u001b[39mis_train,\n\u001b[1;32m     38\u001b[0m         has_upper\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_upper\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     39\u001b[0m     )\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m step_model, step_model\u001b[38;5;241m.\u001b[39mfinish_steps\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/spacy/ml/parser_model.pyx:213\u001b[0m, in \u001b[0;36mspacy.ml.parser_model.ParserStepModel.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m layer(X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m layer(X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "    \u001b[0;31m[... skipping similar frames: Model.__call__ at line 291 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m layer(X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/thinc/layers/with_array.py:37\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, Xseq, is_train)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     34\u001b[0m     model: Model[SeqT, SeqT], Xseq: SeqT, is_train: \u001b[38;5;28mbool\u001b[39m\n\u001b[1;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[SeqT, Callable]:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(Xseq, Ragged):\n\u001b[0;32m---> 37\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], _ragged_forward(model, Xseq, is_train))\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(Xseq, Padded):\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], _padded_forward(model, Xseq, is_train))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/thinc/layers/with_array.py:92\u001b[0m, in \u001b[0;36m_ragged_forward\u001b[0;34m(model, Xr, is_train)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ragged_forward\u001b[39m(\n\u001b[1;32m     89\u001b[0m     model: Model[SeqT, SeqT], Xr: Ragged, is_train: \u001b[38;5;28mbool\u001b[39m\n\u001b[1;32m     90\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Ragged, Callable]:\n\u001b[1;32m     91\u001b[0m     layer: Model[ArrayXd, ArrayXd] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 92\u001b[0m     Y, get_dX \u001b[38;5;241m=\u001b[39m layer(Xr\u001b[38;5;241m.\u001b[39mdataXd, is_train)\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackprop\u001b[39m(dYr: Ragged) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Ragged:\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Ragged(get_dX(dYr\u001b[38;5;241m.\u001b[39mdataXd), dYr\u001b[38;5;241m.\u001b[39mlengths)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m layer(X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m layer(X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/thinc/layers/maxout.py:53\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     51\u001b[0m W \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_param(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m W \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape2f(W, nO \u001b[38;5;241m*\u001b[39m nP, nI)\n\u001b[0;32m---> 53\u001b[0m Y \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mgemm(X, W, trans2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     54\u001b[0m Y \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape1f(b, nO \u001b[38;5;241m*\u001b[39m nP)\n\u001b[1;32m     55\u001b[0m Z \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape3f(Y, Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], nO, nP)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a dictionary to store counts\n",
    "class_counts = {'class_label': [], 'num_sentences': [], 'num_tokens': []}\n",
    "\n",
    "# Iterate through each class\n",
    "for class_label in class_labels:\n",
    "    # Select comments for the current class\n",
    "    class_comments = df[df[class_label] != -1]['comment_text'].tolist()\n",
    "\n",
    "    # Initialize counters\n",
    "    total_sentences = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    # Iterate through comments in the current class\n",
    "    for comment in class_comments:\n",
    "        # Process the comment with spaCy\n",
    "        doc = nlp(comment)\n",
    "\n",
    "        # Count sentences and tokens\n",
    "        total_sentences += len(list(doc.sents))\n",
    "        total_tokens += len(doc)\n",
    "\n",
    "    # Update the counts in the dictionary\n",
    "    class_counts['class_label'].append(class_label)\n",
    "    class_counts['num_sentences'].append(total_sentences)\n",
    "    class_counts['num_tokens'].append(total_tokens)\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "class_counts_df = pd.DataFrame(class_counts)\n",
    "\n",
    "# Display the result\n",
    "print(class_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "1957dae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic Counts:\n",
      "Non-toxic: 144277\n",
      "toxic: 15294\n",
      "\n",
      "Severe_toxic Counts:\n",
      "Non-severe_toxic: 157976\n",
      "severe_toxic: 1595\n",
      "\n",
      "Obscene Counts:\n",
      "Non-obscene: 151122\n",
      "obscene: 8449\n",
      "\n",
      "Threat Counts:\n",
      "Non-threat: 159093\n",
      "threat: 478\n",
      "\n",
      "Insult Counts:\n",
      "Non-insult: 151694\n",
      "insult: 7877\n",
      "\n",
      "Identity_hate Counts:\n",
      "Non-identity_hate: 158166\n",
      "identity_hate: 1405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# EXPLORING CLASS DISTRIBUTION\n",
    "class_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "# Loop through class columns and print class counts\n",
    "for column in class_columns:\n",
    "    class_counts = df[column].value_counts()\n",
    "    \n",
    "    print(f\"{column.capitalize()} Counts:\")\n",
    "    for index, count in class_counts.items():\n",
    "        class_label = \"Non-\" + column if index == 0 else column\n",
    "        print(f\"{class_label}: {count}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba6d799",
   "metadata": {},
   "source": [
    "---\n",
    "## Feature Extraction\n",
    "    \n",
    "### Create a TF-IDF \n",
    "vectoriser = TfTfidfVectorizer()\n",
    "transformed_output = v.fit_transform(\n",
    "print(v.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff2a784",
   "metadata": {},
   "source": [
    "### 2.2 Requirements\n",
    "\n",
    "We're going to review the data and it's Perform detailed data analysis of the dataset provided by the competition, observing:\n",
    "\n",
    "Number of sentences and tokens per class (and check if the dataset is unbalanced or not).\n",
    "\n",
    "Analyse the most common words for each class and, therefore, understand the most used terms for each level of toxicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3027e1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a stylish seaborn theme\n",
    "sns.set_theme()\n",
    "\n",
    "# Visualize the results with a dark palette\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "sns.barplot(x='class_label', y='num_sentences', data=class_counts_df, palette='dark')\n",
    "plt.title('Number of Sentences per Class')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "sns.barplot(x='class_label', y='num_tokens', data=class_counts_df, palette='dark')\n",
    "plt.title('Number of Tokens per Class')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b84d45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
