{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f69ce32",
   "metadata": {},
   "source": [
    "# CSCK507 Mid Module - Toxic Comment Classification Challenge\n",
    "\n",
    "## Table of Contents\n",
    "[Section 1. Introduction](#introduction)\n",
    "- [Import Dependencies](#import-dependencies)\n",
    "- [Initialise SpaCy Model ](#import-dependencies)\n",
    "\n",
    "[Section 2. Data Exploration & Analysis](#data-exploration-&-analysis)\n",
    "  - [2.1 Data Preprocessing](#data-preprocessing)\n",
    "  - [2.2 Data Imbalance](#data-imbalance)\n",
    "  \n",
    "[Section 3. Feature Extraction](#data-exploration-&-analysis)\n",
    "\n",
    "[Section 4. Machine Learning Models](#data-exploration-&-analysis)\n",
    "\n",
    "[Section 5. Model Evaluation](#data-exploration-&-analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a35d39a",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Originating in 2018, this challenge revolves around classifying different levels of toxicity in online comments. The dataset from the inaugural competition is utilized to analyze and evaluate the performance of various machine learning algorithms in categorizing six types of toxicity. The primary goal is not only to find an optimal solution but to understand the process of evaluating machine learning algorithms' performance in a classification task. This individual assessment involves data analysis, algorithm selection, and the exploration of feature extraction methods to uncover insights into the nuances of toxic comment classification.\n",
    "\n",
    "The dataset for the Toxic Comment Classification Challenge can be obtained from the competition page on Kaggle, here: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f040f411",
   "metadata": {},
   "source": [
    "### Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4d3aae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import spacy\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# For Data Preprocessing\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re  \n",
    "\n",
    "# For Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#For Feature Extraction  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string \n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889aaac8",
   "metadata": {},
   "source": [
    "### Loading Kaggle dataset into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "cdd219e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./train.csv')\n",
    "df_test_labels = pd.read_csv('./test_labels.csv')\n",
    "df_test_comments = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5510ef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spacy.prefer_gpu()\n",
    "    spacy.load('en_core_web_sm')\n",
    "except LookupError:\n",
    "    print('Run: python -m spacy download en_core_web_sm')\n",
    "\n",
    "try:\n",
    "    nltk_stop = stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baed09c",
   "metadata": {},
   "source": [
    "### Initialise SpaCy Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9716bf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d038ddd4",
   "metadata": {},
   "source": [
    "## 2. Data Exploration & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e3e85c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   id             159571 non-null  object\n",
      " 1   comment_text   159571 non-null  object\n",
      " 2   toxic          159571 non-null  int64 \n",
      " 3   severe_toxic   159571 non-null  int64 \n",
      " 4   obscene        159571 non-null  int64 \n",
      " 5   threat         159571 non-null  int64 \n",
      " 6   insult         159571 non-null  int64 \n",
      " 7   identity_hate  159571 non-null  int64 \n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 9.7+ MB\n",
      "The table dimensions are: (159571, 8)\n"
     ]
    }
   ],
   "source": [
    "df.info()\n",
    "print(\"The table dimensions are:\",df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "cd3e605e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aefc07",
   "metadata": {},
   "source": [
    "### Dataset Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "53bf8b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_test before removing -1: (63978, 9)\n",
      "df_test after removing -1: (63978, 9)\n"
     ]
    }
   ],
   "source": [
    "# Obtain class labels of the dataset\n",
    "class_labels = list(df.columns[2:])\n",
    "class_labels\n",
    "\n",
    "# Remove rows with -1 from df_test as they are not used for scoring\n",
    "print(f'df_test before removing -1: {df_test.shape}')\n",
    "for class_label in class_labels:\n",
    "    df_test = df_test[df_test[class_label] != -1]\n",
    "print(f'df_test after removing -1: {df_test.shape}')\n",
    "\n",
    "# Left join 'df_test' and 'df_testcomments' on the 'id' column\n",
    "df_test = pd.merge(df_test, df_testcomments, on='id', how='left')\n",
    "\n",
    "# Rearrange columns to match the structure of 'df'\n",
    "df_test = df_test[['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d2825a",
   "metadata": {},
   "source": [
    "### 2.1 Data Preprocessing\n",
    "\n",
    "This part consists of... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50e5572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess a text string.\n",
    "\n",
    "    Operations performed:\n",
    "    - Replace special characters, URLs, and numbers with spaces.\n",
    "    - Remove extra spaces and replace \"\\n\" with a space.\n",
    "    - Remove Non-English characters.\n",
    "    - Remove start and end white spaces.\n",
    "    - Remove single characters.\n",
    "    - Remove punctuations.\n",
    "    - Convert the text to lowercase.\n",
    "    - Remove common stopwords.\n",
    "\n",
    "    :param text: Input text (string).\n",
    "    :return: Cleaned text (string).\n",
    "\n",
    "    Example:\n",
    "    >>> input_text = \"An example text with special characters: $100 and URLs like https://example.com.\"\n",
    "    >>> preprocess_text(input_text)\n",
    "    'example text special characters URLs like'\n",
    "    \"\"\"\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    # Remove extra spaces and replace \"\\n\" with a space\n",
    "    text = re.sub(\"\\s\\s+\", \" \", text).replace(\"\\n\", \" \")\n",
    "    # Remove Non-English characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', \"\", text)\n",
    "    # Remove start and end white spaces\n",
    "    text = text.strip()\n",
    "    # Remove single characters\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", text)\n",
    "    # Remove punctuations\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]+\", \" \", text)\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Stopword Removal\n",
    "    text = ' '.join([word for word in text.split() if word not in nltk_stop])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d0c244",
   "metadata": {},
   "source": [
    "### Tokenisation and Lemmatisation\n",
    "\n",
    "This code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bb6e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(documents):\n",
    "    \"\"\"\n",
    "    Tokenize a list of documents and perform the following:\n",
    "    1. Break text into individual words or subword tokens.\n",
    "    2. Reduce words to their base or root form using lemmatization.\n",
    "    3. Remove stop words and non-alphabetic characters.\n",
    "\n",
    "    Utilises spaCy's nlp.pipe for efficient batch processing.\n",
    "\n",
    "    :param documents: List of strings representing documents.\n",
    "    :return: List of lists of strings, where each list corresponds to the lemmatized tokens of a document.\n",
    "    \"\"\"\n",
    "    lemmatized_tokens_list = []\n",
    "    \n",
    "    # Process documents using spaCy's nlp.pipe with \"ner\" and \"parser\" components disabled\n",
    "    for doc in nlp.pipe(documents, disable=[\"ner\", \"parser\"], batch_size=5000):\n",
    "        # Generate lemmatised tokens, remove stop words, and non-alphabetic characters\n",
    "        lemmatized_tokens = [token.lemma_ for token in doc if token.is_alpha and token.lemma_ not in nlp.Defaults.stop_words]\n",
    "        lemmatized_tokens_list.append(lemmatized_tokens)\n",
    "\n",
    "    return lemmatized_tokens_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4c76cc",
   "metadata": {},
   "source": [
    "In many tokenization tasks, especially when you're primarily interested in lemmatization and removing stop words, you may not need the additional information provided by the \"ner\" and \"parser\" components.\n",
    "\n",
    "Disabling the \"ner\" and \"parser\" components during the processing of documents with nlp.pipe will reduce computational laod and can significantly improve efficiency and speed, especially when dealing with a large amount of text data.\n",
    "\n",
    "It's a trade-off between computational resources and the specific linguistic information your task requires. If named entities and syntactic parsing are not critical for your task, disabling these components is a pragmatic approach to enhance processing speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1c7971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to train data\n",
    "df['comment_text'] = df['comment_text'].apply(preprocess_text)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deb9a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to test data\n",
    "df_test['comment_text'] = df_test['comment_text'].apply(preprocess_text)\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1957dae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic Counts:\n",
      "Non-toxic: 144277\n",
      "toxic: 15294\n",
      "\n",
      "Severe_toxic Counts:\n",
      "Non-severe_toxic: 157976\n",
      "severe_toxic: 1595\n",
      "\n",
      "Obscene Counts:\n",
      "Non-obscene: 151122\n",
      "obscene: 8449\n",
      "\n",
      "Threat Counts:\n",
      "Non-threat: 159093\n",
      "threat: 478\n",
      "\n",
      "Insult Counts:\n",
      "Non-insult: 151694\n",
      "insult: 7877\n",
      "\n",
      "Identity_hate Counts:\n",
      "Non-identity_hate: 158166\n",
      "identity_hate: 1405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# EXPLORING CLASS DISTRIBUTION\n",
    "class_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "# Loop through class columns and print class counts\n",
    "for column in class_columns:\n",
    "    class_counts = df[column].value_counts()\n",
    "    \n",
    "    print(f\"{column.capitalize()} Counts:\")\n",
    "    for index, count in class_counts.items():\n",
    "        class_label = \"Non-\" + column if index == 0 else column\n",
    "        print(f\"{class_label}: {count}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba6d799",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "    \n",
    "### Create a TF-IDF \n",
    "vectoriser = TfTfidfVectorizer()\n",
    "transformed_output = v.fit_transform(\n",
    "print(v.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff2a784",
   "metadata": {},
   "source": [
    "### 2.2 Requirements\n",
    "\n",
    "We're going to review the data and it's Perform detailed data analysis of the dataset provided by the competition, observing:\n",
    "\n",
    "Number of sentences and tokens per class (and check if the dataset is unbalanced or not).\n",
    "\n",
    "Analyse the most common words for each class and, therefore, understand the most used terms for each level of toxicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a87d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of stences and tokens per class using SpaCy.\n",
    "# Assuming you have a DataFrame 'df' with a column 'comment_text' and a column 'class_label'\n",
    "class_labels = df['class_label'].unique()\n",
    "\n",
    "# Create a dictionary to store counts\n",
    "class_counts = {'class_label': [], 'num_sentences': [], 'num_tokens': []}\n",
    "\n",
    "# Iterate through each class\n",
    "for class_label in class_labels:\n",
    "    # Select comments for the current class\n",
    "    class_comments = df[df['class_label'] == class_label]['comment_text'].tolist()\n",
    "\n",
    "    # Initialize counters\n",
    "    total_sentences = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    # Iterate through comments in the current class\n",
    "    for comment in class_comments:\n",
    "        # Process the comment with spaCy\n",
    "        doc = nlp(comment)\n",
    "\n",
    "        # Count sentences and tokens\n",
    "        total_sentences += len(list(doc.sents))\n",
    "        total_tokens += len(doc)\n",
    "\n",
    "    # Update the counts in the dictionary\n",
    "    class_counts['class_label'].append(class_label)\n",
    "    class_counts['num_sentences'].append(total_sentences)\n",
    "    class_counts['num_tokens'].append(total_tokens)\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "class_counts_df = pd.DataFrame(class_counts)\n",
    "\n",
    "# Display the result\n",
    "print(class_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3027e1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a stylish seaborn theme\n",
    "sns.set_theme()\n",
    "\n",
    "# Visualize the results with a dark palette\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "sns.barplot(x='class_label', y='num_sentences', data=class_counts_df, palette='dark')\n",
    "plt.title('Number of Sentences per Class')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "sns.barplot(x='class_label', y='num_tokens', data=class_counts_df, palette='dark')\n",
    "plt.title('Number of Tokens per Class')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b84d45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
